{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "\n",
    "# Sklearn ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to control pipeline\n",
    "normalize_bool = 0\n",
    "outlier_bool = 0\n",
    "resample_bool = 0\n",
    "feature_selection = 0\n",
    "dim_reduction_bool = 0\n",
    "\n",
    "# Initialize variable for first test/train split\n",
    "test_percent = 0.15\n",
    "\n",
    "# Use same random seed to ensure same results across runs\n",
    "rand_seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZYURRE527</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZWNWBP435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZVHEZA963</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZSFNU1100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZRXUB1049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>AGHXWX765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>AFEOPC672</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>AEEEIG737</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>ADQRPH513</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>ABNTSS552</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid  class\n",
       "0    ZYURRE527      4\n",
       "1    ZWNWBP435      0\n",
       "2    ZVHEZA963      4\n",
       "3    ZSFNU1100      4\n",
       "4    ZRXUB1049      0\n",
       "..         ...    ...\n",
       "422  AGHXWX765      0\n",
       "423  AFEOPC672      3\n",
       "424  AEEEIG737      3\n",
       "425  ADQRPH513      3\n",
       "426  ABNTSS552      4\n",
       "\n",
       "[427 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "filepath = \"data/train.csv\"\n",
    "class_df = pd.read_csv(\n",
    "    filepath, usecols=[1, 2], header=0, names=[\"uid\", \"class\"]\n",
    ")\n",
    "\n",
    "display(class_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>-0.001031</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>-0.113097</td>\n",
       "      <td>-0.284965</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.271864</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680631</td>\n",
       "      <td>-1.153061</td>\n",
       "      <td>0.111816</td>\n",
       "      <td>0.162622</td>\n",
       "      <td>-1.085265</td>\n",
       "      <td>-0.657002</td>\n",
       "      <td>-1.406191</td>\n",
       "      <td>2.240085</td>\n",
       "      <td>0.118616</td>\n",
       "      <td>-0.728013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>-0.045820</td>\n",
       "      <td>-0.216762</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>-0.001331</td>\n",
       "      <td>-0.465898</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.241972</td>\n",
       "      <td>-0.115316</td>\n",
       "      <td>-0.411191</td>\n",
       "      <td>0.431461</td>\n",
       "      <td>0.442649</td>\n",
       "      <td>1.243681</td>\n",
       "      <td>-0.151721</td>\n",
       "      <td>0.458508</td>\n",
       "      <td>1.931918</td>\n",
       "      <td>-0.241081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.083430</td>\n",
       "      <td>-0.292385</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.236576</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659314</td>\n",
       "      <td>-0.792833</td>\n",
       "      <td>-0.471358</td>\n",
       "      <td>0.514799</td>\n",
       "      <td>-0.846220</td>\n",
       "      <td>0.479314</td>\n",
       "      <td>-0.730218</td>\n",
       "      <td>1.352716</td>\n",
       "      <td>0.040223</td>\n",
       "      <td>-0.163302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>-0.109248</td>\n",
       "      <td>-0.183284</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.001447</td>\n",
       "      <td>-0.066267</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047666</td>\n",
       "      <td>-0.201043</td>\n",
       "      <td>-0.565545</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>-0.332314</td>\n",
       "      <td>-0.066972</td>\n",
       "      <td>-1.263785</td>\n",
       "      <td>3.876905</td>\n",
       "      <td>-0.397950</td>\n",
       "      <td>-0.693763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.068301</td>\n",
       "      <td>-0.283487</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>-0.251112</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221178</td>\n",
       "      <td>-0.253239</td>\n",
       "      <td>-0.046740</td>\n",
       "      <td>0.242367</td>\n",
       "      <td>-0.379724</td>\n",
       "      <td>-0.893249</td>\n",
       "      <td>-0.957397</td>\n",
       "      <td>1.118245</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.024197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>-0.093583</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-0.367352</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260746</td>\n",
       "      <td>-0.741712</td>\n",
       "      <td>-0.887129</td>\n",
       "      <td>0.190525</td>\n",
       "      <td>0.216271</td>\n",
       "      <td>0.490549</td>\n",
       "      <td>-1.047399</td>\n",
       "      <td>1.875185</td>\n",
       "      <td>0.345561</td>\n",
       "      <td>-0.874318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.006178</td>\n",
       "      <td>-0.000811</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>-0.108863</td>\n",
       "      <td>-0.302020</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>-0.197981</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457373</td>\n",
       "      <td>-0.782917</td>\n",
       "      <td>-1.072765</td>\n",
       "      <td>1.180279</td>\n",
       "      <td>-0.111142</td>\n",
       "      <td>1.897755</td>\n",
       "      <td>-0.902370</td>\n",
       "      <td>0.552967</td>\n",
       "      <td>-0.314270</td>\n",
       "      <td>-1.198762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>-0.152744</td>\n",
       "      <td>-0.355706</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>-0.320724</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>0.232481</td>\n",
       "      <td>-0.527885</td>\n",
       "      <td>-0.305296</td>\n",
       "      <td>-0.189008</td>\n",
       "      <td>-0.592684</td>\n",
       "      <td>-1.144780</td>\n",
       "      <td>3.459698</td>\n",
       "      <td>-0.199579</td>\n",
       "      <td>-0.999165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>-0.092386</td>\n",
       "      <td>-0.434045</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>-0.228858</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147889</td>\n",
       "      <td>1.168724</td>\n",
       "      <td>-0.486698</td>\n",
       "      <td>1.134707</td>\n",
       "      <td>-0.029372</td>\n",
       "      <td>0.092189</td>\n",
       "      <td>-0.791921</td>\n",
       "      <td>1.786787</td>\n",
       "      <td>2.089036</td>\n",
       "      <td>-0.690614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.047733</td>\n",
       "      <td>-0.071875</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>-0.193959</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064665</td>\n",
       "      <td>-0.444786</td>\n",
       "      <td>-0.879349</td>\n",
       "      <td>1.048909</td>\n",
       "      <td>0.213126</td>\n",
       "      <td>1.170847</td>\n",
       "      <td>-1.172747</td>\n",
       "      <td>1.686595</td>\n",
       "      <td>0.482523</td>\n",
       "      <td>-0.440434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows Ã— 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.000462  0.005583 -0.001031  0.002307 -0.113097 -0.284965  0.001069   \n",
       "1    0.000220  0.006780 -0.000547  0.002183 -0.045820 -0.216762  0.000987   \n",
       "2    0.000405  0.007183 -0.000137  0.002612 -0.083430 -0.292385  0.001094   \n",
       "3    0.000388  0.003802  0.002121  0.001513 -0.109248 -0.183284  0.000813   \n",
       "4    0.000425  0.006544  0.001630  0.001549 -0.068301 -0.283487  0.001004   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "422  0.000305  0.003671 -0.004093  0.003010 -0.093583  0.133018  0.000627   \n",
       "423  0.000441  0.006178 -0.000811  0.003572 -0.108863 -0.302020  0.000761   \n",
       "424  0.000464  0.006611  0.000842  0.001412 -0.152744 -0.355706  0.000906   \n",
       "425  0.000233  0.003029  0.001606  0.001224 -0.092386 -0.434045  0.000668   \n",
       "426  0.000233  0.006601 -0.001318  0.001098 -0.047733 -0.071875  0.000654   \n",
       "\n",
       "         7         8         9     ...      1014      1015      1016  \\\n",
       "0   -0.000092 -0.271864  0.000503  ...  0.680631 -1.153061  0.111816   \n",
       "1   -0.001331 -0.465898  0.000515  ... -1.241972 -0.115316 -0.411191   \n",
       "2   -0.000112 -0.236576  0.000466  ...  0.659314 -0.792833 -0.471358   \n",
       "3   -0.001447 -0.066267  0.000654  ... -0.047666 -0.201043 -0.565545   \n",
       "4   -0.001800 -0.251112  0.000428  ... -1.221178 -0.253239 -0.046740   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "422  0.001443 -0.367352  0.000462  ... -0.260746 -0.741712 -0.887129   \n",
       "423  0.001851 -0.197981  0.000310  ...  0.457373 -0.782917 -1.072765   \n",
       "424 -0.001229 -0.320724  0.000493  ...  0.411773  0.232481 -0.527885   \n",
       "425 -0.000410 -0.228858  0.000444  ... -0.147889  1.168724 -0.486698   \n",
       "426  0.000902 -0.193959  0.000319  ... -0.064665 -0.444786 -0.879349   \n",
       "\n",
       "         1017      1018      1019      1020      1021      1022      1023  \n",
       "0    0.162622 -1.085265 -0.657002 -1.406191  2.240085  0.118616 -0.728013  \n",
       "1    0.431461  0.442649  1.243681 -0.151721  0.458508  1.931918 -0.241081  \n",
       "2    0.514799 -0.846220  0.479314 -0.730218  1.352716  0.040223 -0.163302  \n",
       "3    0.999009 -0.332314 -0.066972 -1.263785  3.876905 -0.397950 -0.693763  \n",
       "4    0.242367 -0.379724 -0.893249 -0.957397  1.118245  0.181925 -0.024197  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "422  0.190525  0.216271  0.490549 -1.047399  1.875185  0.345561 -0.874318  \n",
       "423  1.180279 -0.111142  1.897755 -0.902370  0.552967 -0.314270 -1.198762  \n",
       "424 -0.305296 -0.189008 -0.592684 -1.144780  3.459698 -0.199579 -0.999165  \n",
       "425  1.134707 -0.029372  0.092189 -0.791921  1.786787  2.089036 -0.690614  \n",
       "426  1.048909  0.213126  1.170847 -1.172747  1.686595  0.482523 -0.440434  \n",
       "\n",
       "[427 rows x 1024 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load features from individual CSVs into a single dataframe\n",
    "def get_features(uid):\n",
    "    feature_filepath = f\"data/img_details/{uid}.csv\"\n",
    "    feature_df = pd.read_csv(feature_filepath, header=None)\n",
    "    return feature_df.iloc[0].values.tolist()\n",
    "\n",
    "\n",
    "features_df = class_df[[\"uid\"]].apply(\n",
    "    lambda row: get_features(row[0]), axis=1, result_type=\"expand\"\n",
    ")\n",
    "display(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>class</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f1014</th>\n",
       "      <th>f1015</th>\n",
       "      <th>f1016</th>\n",
       "      <th>f1017</th>\n",
       "      <th>f1018</th>\n",
       "      <th>f1019</th>\n",
       "      <th>f1020</th>\n",
       "      <th>f1021</th>\n",
       "      <th>f1022</th>\n",
       "      <th>f1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZYURRE527</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>-0.001031</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>-0.113097</td>\n",
       "      <td>-0.284965</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680631</td>\n",
       "      <td>-1.153061</td>\n",
       "      <td>0.111816</td>\n",
       "      <td>0.162622</td>\n",
       "      <td>-1.085265</td>\n",
       "      <td>-0.657002</td>\n",
       "      <td>-1.406191</td>\n",
       "      <td>2.240085</td>\n",
       "      <td>0.118616</td>\n",
       "      <td>-0.728013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZWNWBP435</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>-0.045820</td>\n",
       "      <td>-0.216762</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>-0.001331</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.241972</td>\n",
       "      <td>-0.115316</td>\n",
       "      <td>-0.411191</td>\n",
       "      <td>0.431461</td>\n",
       "      <td>0.442649</td>\n",
       "      <td>1.243681</td>\n",
       "      <td>-0.151721</td>\n",
       "      <td>0.458508</td>\n",
       "      <td>1.931918</td>\n",
       "      <td>-0.241081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZVHEZA963</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.083430</td>\n",
       "      <td>-0.292385</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659314</td>\n",
       "      <td>-0.792833</td>\n",
       "      <td>-0.471358</td>\n",
       "      <td>0.514799</td>\n",
       "      <td>-0.846220</td>\n",
       "      <td>0.479314</td>\n",
       "      <td>-0.730218</td>\n",
       "      <td>1.352716</td>\n",
       "      <td>0.040223</td>\n",
       "      <td>-0.163302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZSFNU1100</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>-0.109248</td>\n",
       "      <td>-0.183284</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.001447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047666</td>\n",
       "      <td>-0.201043</td>\n",
       "      <td>-0.565545</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>-0.332314</td>\n",
       "      <td>-0.066972</td>\n",
       "      <td>-1.263785</td>\n",
       "      <td>3.876905</td>\n",
       "      <td>-0.397950</td>\n",
       "      <td>-0.693763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZRXUB1049</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.068301</td>\n",
       "      <td>-0.283487</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221178</td>\n",
       "      <td>-0.253239</td>\n",
       "      <td>-0.046740</td>\n",
       "      <td>0.242367</td>\n",
       "      <td>-0.379724</td>\n",
       "      <td>-0.893249</td>\n",
       "      <td>-0.957397</td>\n",
       "      <td>1.118245</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.024197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>AGHXWX765</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>-0.093583</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260746</td>\n",
       "      <td>-0.741712</td>\n",
       "      <td>-0.887129</td>\n",
       "      <td>0.190525</td>\n",
       "      <td>0.216271</td>\n",
       "      <td>0.490549</td>\n",
       "      <td>-1.047399</td>\n",
       "      <td>1.875185</td>\n",
       "      <td>0.345561</td>\n",
       "      <td>-0.874318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>AFEOPC672</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.006178</td>\n",
       "      <td>-0.000811</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>-0.108863</td>\n",
       "      <td>-0.302020</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457373</td>\n",
       "      <td>-0.782917</td>\n",
       "      <td>-1.072765</td>\n",
       "      <td>1.180279</td>\n",
       "      <td>-0.111142</td>\n",
       "      <td>1.897755</td>\n",
       "      <td>-0.902370</td>\n",
       "      <td>0.552967</td>\n",
       "      <td>-0.314270</td>\n",
       "      <td>-1.198762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>AEEEIG737</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>-0.152744</td>\n",
       "      <td>-0.355706</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>0.232481</td>\n",
       "      <td>-0.527885</td>\n",
       "      <td>-0.305296</td>\n",
       "      <td>-0.189008</td>\n",
       "      <td>-0.592684</td>\n",
       "      <td>-1.144780</td>\n",
       "      <td>3.459698</td>\n",
       "      <td>-0.199579</td>\n",
       "      <td>-0.999165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>ADQRPH513</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>-0.092386</td>\n",
       "      <td>-0.434045</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147889</td>\n",
       "      <td>1.168724</td>\n",
       "      <td>-0.486698</td>\n",
       "      <td>1.134707</td>\n",
       "      <td>-0.029372</td>\n",
       "      <td>0.092189</td>\n",
       "      <td>-0.791921</td>\n",
       "      <td>1.786787</td>\n",
       "      <td>2.089036</td>\n",
       "      <td>-0.690614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>ABNTSS552</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.047733</td>\n",
       "      <td>-0.071875</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064665</td>\n",
       "      <td>-0.444786</td>\n",
       "      <td>-0.879349</td>\n",
       "      <td>1.048909</td>\n",
       "      <td>0.213126</td>\n",
       "      <td>1.170847</td>\n",
       "      <td>-1.172747</td>\n",
       "      <td>1.686595</td>\n",
       "      <td>0.482523</td>\n",
       "      <td>-0.440434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows Ã— 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid  class        f0        f1        f2        f3        f4  \\\n",
       "0    ZYURRE527      4  0.000462  0.005583 -0.001031  0.002307 -0.113097   \n",
       "1    ZWNWBP435      0  0.000220  0.006780 -0.000547  0.002183 -0.045820   \n",
       "2    ZVHEZA963      4  0.000405  0.007183 -0.000137  0.002612 -0.083430   \n",
       "3    ZSFNU1100      4  0.000388  0.003802  0.002121  0.001513 -0.109248   \n",
       "4    ZRXUB1049      0  0.000425  0.006544  0.001630  0.001549 -0.068301   \n",
       "..         ...    ...       ...       ...       ...       ...       ...   \n",
       "422  AGHXWX765      0  0.000305  0.003671 -0.004093  0.003010 -0.093583   \n",
       "423  AFEOPC672      3  0.000441  0.006178 -0.000811  0.003572 -0.108863   \n",
       "424  AEEEIG737      3  0.000464  0.006611  0.000842  0.001412 -0.152744   \n",
       "425  ADQRPH513      3  0.000233  0.003029  0.001606  0.001224 -0.092386   \n",
       "426  ABNTSS552      4  0.000233  0.006601 -0.001318  0.001098 -0.047733   \n",
       "\n",
       "           f5        f6        f7  ...     f1014     f1015     f1016  \\\n",
       "0   -0.284965  0.001069 -0.000092  ...  0.680631 -1.153061  0.111816   \n",
       "1   -0.216762  0.000987 -0.001331  ... -1.241972 -0.115316 -0.411191   \n",
       "2   -0.292385  0.001094 -0.000112  ...  0.659314 -0.792833 -0.471358   \n",
       "3   -0.183284  0.000813 -0.001447  ... -0.047666 -0.201043 -0.565545   \n",
       "4   -0.283487  0.001004 -0.001800  ... -1.221178 -0.253239 -0.046740   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "422  0.133018  0.000627  0.001443  ... -0.260746 -0.741712 -0.887129   \n",
       "423 -0.302020  0.000761  0.001851  ...  0.457373 -0.782917 -1.072765   \n",
       "424 -0.355706  0.000906 -0.001229  ...  0.411773  0.232481 -0.527885   \n",
       "425 -0.434045  0.000668 -0.000410  ... -0.147889  1.168724 -0.486698   \n",
       "426 -0.071875  0.000654  0.000902  ... -0.064665 -0.444786 -0.879349   \n",
       "\n",
       "        f1017     f1018     f1019     f1020     f1021     f1022     f1023  \n",
       "0    0.162622 -1.085265 -0.657002 -1.406191  2.240085  0.118616 -0.728013  \n",
       "1    0.431461  0.442649  1.243681 -0.151721  0.458508  1.931918 -0.241081  \n",
       "2    0.514799 -0.846220  0.479314 -0.730218  1.352716  0.040223 -0.163302  \n",
       "3    0.999009 -0.332314 -0.066972 -1.263785  3.876905 -0.397950 -0.693763  \n",
       "4    0.242367 -0.379724 -0.893249 -0.957397  1.118245  0.181925 -0.024197  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "422  0.190525  0.216271  0.490549 -1.047399  1.875185  0.345561 -0.874318  \n",
       "423  1.180279 -0.111142  1.897755 -0.902370  0.552967 -0.314270 -1.198762  \n",
       "424 -0.305296 -0.189008 -0.592684 -1.144780  3.459698 -0.199579 -0.999165  \n",
       "425  1.134707 -0.029372  0.092189 -0.791921  1.786787  2.089036 -0.690614  \n",
       "426  1.048909  0.213126  1.170847 -1.172747  1.686595  0.482523 -0.440434  \n",
       "\n",
       "[427 rows x 1026 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge dataframes and fix column names\n",
    "num_features = features_df.shape[1]\n",
    "feature_names = [f\"f{i}\" for i in range(num_features)]\n",
    "\n",
    "features_df.columns = feature_names\n",
    "\n",
    "df = pd.concat([class_df, features_df], axis=1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set by class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    135\n",
       "4     92\n",
       "3     66\n",
       "1     46\n",
       "2     23\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set by class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    24\n",
       "4    17\n",
       "3    12\n",
       "1     8\n",
       "2     4\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all = df[feature_names]\n",
    "y_all = df[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=test_percent, random_state=rand_seed, stratify=y_all\n",
    ")\n",
    "\n",
    "# Reset X_train index\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"Training set by class:\")\n",
    "display(y_train.value_counts())\n",
    "print(\"Test set by class:\")\n",
    "display(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data\n",
    "\n",
    "Use MaxAbsScaler from sklearn. This scales each feature by its maximum absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that will normalize the X_train data\n",
    "def norm_data(X_train):\n",
    "    # Setup scaler\n",
    "    #scaler_std = StandardScaler()\n",
    "    scaler_abs = MaxAbsScaler()\n",
    "    \n",
    "    # Apply scaling to training data\n",
    "    X_train = scaler_abs.fit_transform(X_train)\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test norm_data function\n",
    "#X_train = norm_data(X_train)\n",
    "\n",
    "#print(type(X_train))\n",
    "##print(normalize_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "\n",
    "Standard deviation approach - For each class, features with values more than +/- 3 standard deviations away from the per class per feature mean will be removed.\n",
    "\n",
    "Since we are in multi-dimensional space, we will use the mean and covariance matrices. This will be computed using Mahalanobis distance which is well-suited for multi-dimensional space: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.mahalanobis.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that computes mean, cov matrix, and inv cov matrix\n",
    "def get_mean_cov(X_train):\n",
    "    # Merge dfs\n",
    "    norm_x_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "    norm_df = pd.concat([y_train, norm_x_df], axis=1)\n",
    "\n",
    "    # Compute mean and cov per class per feature\n",
    "    avg_list = []\n",
    "    cov_list = []\n",
    "    inv_cov_list = []\n",
    "    for i in range(5):\n",
    "        # Compute mean\n",
    "        avg = np.mean(norm_df[norm_df[\"class\"]==i][feature_names], axis=0)\n",
    "        avg_list.append(avg)\n",
    "        # Compute cov matrix\n",
    "        cov = np.cov(norm_df[norm_df[\"class\"]==i][feature_names], rowvar=False)\n",
    "        cov_list.append(cov)\n",
    "        # Compute inverse of cov matrix\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "        inv_cov_list.append(inv_cov)\n",
    "    return norm_df, avg_list, inv_cov_list\n",
    "\n",
    "    #display(avg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test get_mean_cov function\n",
    "#norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    290.000000\n",
       "mean      20.312575\n",
       "std        8.401853\n",
       "min        1.322598\n",
       "25%       14.067607\n",
       "50%       20.792200\n",
       "75%       26.471778\n",
       "max       37.066707\n",
       "Name: mahalanobis_dist, dtype: float64"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which features should be removed (identify outliers based on Mahalanobis dist)\n",
    "# Create function that computes Mahalanobis distance and adds it to norm_df\n",
    "def get_mahalanobis_dist(label, features):\n",
    "    u = avg_list[label]\n",
    "    v = features\n",
    "    vi = inv_cov_list[label]\n",
    "    delta = u - v\n",
    "    m = np.dot(np.dot(delta, vi), delta)\n",
    "    #dist = distance.mahalanobis(u, features, vi)\n",
    "    return np.sqrt(np.abs(m))\n",
    "\n",
    "# Call function for each feature\n",
    "norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "norm_df[\"mahalanobis_dist\"].describe()\n",
    "\n",
    "# Initialize list for distances\n",
    "# dist = np.zeros(norm_df.shape[0])\n",
    "# for i, row in norm_df.iterrows():\n",
    "#     dist[i] = get_mahalanobis_dist(int(row[\"class\"]), row[feature_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances seem high, but we may have a lot of outliers! Plus there are so many features that the sum becomes large. The distance was calculated for all 362 image feature vectors in the training dataset, so we can move on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers\n",
    "def drop_outliers(norm_df, threshold):\n",
    "    thresh = threshold\n",
    "    norm_df.sort_values(by=\"mahalanobis_dist\", ascending=False, inplace=True)\n",
    "    norm_df.reset_index(inplace=True, drop=True)\n",
    "    norm_df.drop(norm_df.index[:int(norm_df.shape[0]*thresh)], inplace=True)\n",
    "    norm_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test drop_outliers function\n",
    "#norm_df = drop_outliers(norm_df, 0.2)\n",
    "\n",
    "# Print updated descriptive stats\n",
    "#norm_df[\"mahalanobis_dist\"].describe()\n",
    "#print(len(norm_df[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class counts after dropping outliers\n",
    "#print(norm_df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that oversamples or undersamples data\n",
    "def resample(sampler, X_train, y_train, name):\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "    # Observe number of classes after resample\n",
    "    #print(f\"Number of samples per class after {name}:\\n{y_train.value_counts()}\")\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test resample function\n",
    "# Setup ADASYN (oversampling)\n",
    "ada = ADASYN(random_state=rand_seed)\n",
    "# Setup random undersampling\n",
    "undersample = RandomUnderSampler()\n",
    "\n",
    "# Call resample function\n",
    "#X_train, y_train = resample(undersample, X_train, y_train, \"Random Undersampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality - Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis to reduce dimensionality of problem\n",
    "def use_lda(X_train, y_train):\n",
    "    lda_clf = LinearDiscriminantAnalysis()\n",
    "    lda_model = lda_clf.fit(X_train, y_train)\n",
    "    X_lda = lda_model.transform(X_train)\n",
    "    return X_lda, lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test use_lda function\n",
    "#X_lda, lda_model = use_lda(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LDA results\n",
    "def plot_lda(X_lda, y_train):\n",
    "    # Visualize LDA results\n",
    "    target_class = np.unique(y_train)\n",
    "    colors = [\"red\", \"green\", \"blue\", \"black\", \"brown\"]\n",
    "    # Get number of components\n",
    "    n_components = X_lda.shape[1]\n",
    "    pairs = []\n",
    "    for xi in range(n_components):\n",
    "        for yi in range(n_components):\n",
    "            if yi > xi:\n",
    "                pairs.append((xi,yi))\n",
    "\n",
    "    # Setup subplots\n",
    "    fig, ax = plt.subplots(len(pairs), 1, figsize=(10,30))\n",
    "\n",
    "    # Get every combination of LDA components\n",
    "    # Setting up all values to plot\n",
    "    for ax_i,(xi,yi) in enumerate(pairs):\n",
    "        # Plot data\n",
    "        for color, i, c in zip(colors, [0,1,2,3,4], target_class):\n",
    "            ax[ax_i].scatter(X_lda[y_train == i, xi],\n",
    "                X_lda[y_train == i, yi],\n",
    "                alpha=.8, color=color, label=c)\n",
    "        # Add legend\n",
    "        ax[ax_i].set_xlabel(f\"Component {xi}\")\n",
    "        ax[ax_i].set_ylabel(f\"Component {yi}\")\n",
    "        ax[ax_i].legend(loc=\"best\")\n",
    "\n",
    "    # Set title on the plot\n",
    "    #plt.title(\"Linear Discriminant Analysis\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plot_lda function\n",
    "#plot_lda(X_lda, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVM Classifier with Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "# PIPELINE 1: NORMALIZE, DROP OUTLIERS, UNDERSAMPLE, LDA\n",
    "# Normalize\n",
    "X_train = norm_data(X_train)\n",
    "normalize_bool = 1\n",
    "\n",
    "# Drop outliers\n",
    "norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)\n",
    "norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "outlier_thresh = 0.2\n",
    "norm_df = drop_outliers(norm_df, outlier_thresh)\n",
    "outlier_bool = 1\n",
    "\n",
    "# Resample\n",
    "undersample = RandomUnderSampler()\n",
    "X_train = norm_df[feature_names]\n",
    "y_train = norm_df[\"class\"]\n",
    "X_train, y_train = resample(undersample, X_train, y_train, \"Random Undersampling\")\n",
    "resample_bool = 1\n",
    "\n",
    "# LDA\n",
    "X_lda, lda_model = use_lda(X_train, y_train)\n",
    "dim_reduction_bool = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 2: NORMALIZE, DROP OUTLIERS, LDA\n",
    "# Normalize\n",
    "X_train = norm_data(X_train)\n",
    "normalize_bool = 1\n",
    "\n",
    "# Drop outliers\n",
    "norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)\n",
    "norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "outlier_thresh = 0.2\n",
    "norm_df = drop_outliers(norm_df, outlier_thresh)\n",
    "outlier_bool = 1\n",
    "\n",
    "# LDA\n",
    "X_lda, lda_model = use_lda(X_train, y_train)\n",
    "dim_reduction_bool = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 3: NORMALIZE, DROP OUTLIERS\n",
    "# Normalize\n",
    "X_train = norm_data(X_train)\n",
    "normalize_bool = 1\n",
    "\n",
    "# Drop outliers\n",
    "norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)\n",
    "norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "outlier_thresh = 0.2\n",
    "norm_df = drop_outliers(norm_df, outlier_thresh)\n",
    "outlier_bool = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heath\\anaconda3\\envs\\pattern-classification\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but LinearDiscriminantAnalysis was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run LDA on X test data to match dimensionality of train data\n",
    "#X_test_lda = lda_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.369\n",
      "F1 score: 0.199\n",
      "Precision: 0.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heath\\anaconda3\\envs\\pattern-classification\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\heath\\anaconda3\\envs\\pattern-classification\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Use components to train SVM classifier\n",
    "svm_clf = SVC(kernel=\"rbf\", random_state=10)\n",
    "svm_model = svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Use cross_validate from sklearn to obtain accuracies for 5-fold cross validation\n",
    "#cv_results = cross_validate(svm_clf, X_train, y_train, cv=5,\n",
    "#    scoring='accuracy')\n",
    "\n",
    "# Print accuracy for each fold\n",
    "#for i in range(0,5):\n",
    "#    print(f\"The accuracy for fold {i} is: {cv_results['test_score'][i]:.3f}\")\n",
    "    \n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"F1 score: {f1:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x12ddd4128f0>"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    F1Score  Accuracy  Precision    Recall  TestSplit  KFold  Normalized  \\\n",
      "0  0.199136  0.369231   0.136331  0.369231       0.15    0.0         1.0   \n",
      "\n",
      "   OutlierRemoval  OutlierThreshold  Resample  ReduceDimension  \n",
      "0             1.0               0.2       0.0              0.0  \n"
     ]
    }
   ],
   "source": [
    "# Save all results to a DataFrame\n",
    "# First, initialize dataframe\n",
    "pipeline_results_df = pd.DataFrame(columns=[\"F1Score\", \"Accuracy\", \"Precision\", \"Recall\",\n",
    "    \"TestSplit\", \"KFold\", \"Normalized\", \"OutlierRemoval\", \"OutlierThreshold\",\n",
    "    \"Resample\", \"ReduceDimension\"])\n",
    "\n",
    "pipeline_results_df.loc[0] = [f1, acc, precision, recall, test_percent, 0, normalize_bool,\n",
    "    outlier_bool, outlier_thresh, resample_bool, dim_reduction_bool]  \n",
    "\n",
    "print(pipeline_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to csv\n",
    "# Check if csv exists\n",
    "file_path = \"outputs/pipeline_results.csv\"\n",
    "if os.path.isfile(file_path):\n",
    "    # Append new results\n",
    "    pipeline_results_df.to_csv(file_path, mode=\"a\", header=False)\n",
    "else:\n",
    "    # Create new csv and add results\n",
    "    pipeline_results_df.to_csv(file_path, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pattern-classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1895724e0ba1d87308f72752509c0d197b6cd14cd38e9b4860259e222d188bca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
