{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "\n",
    "# Sklearn ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to control pipeline\n",
    "normalize_bool = 0\n",
    "outlier_bool = 0\n",
    "resample_bool = 0\n",
    "feature_selection = 0\n",
    "dim_reduction_bool = 0\n",
    "\n",
    "# Initialize variable for first test/train split\n",
    "test_percent = 0.15\n",
    "\n",
    "# Use same random seed to ensure same results across runs\n",
    "rand_seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZYURRE527</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZWNWBP435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZVHEZA963</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZSFNU1100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZRXUB1049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>AGHXWX765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>AFEOPC672</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>AEEEIG737</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>ADQRPH513</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>ABNTSS552</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid  class\n",
       "0    ZYURRE527      4\n",
       "1    ZWNWBP435      0\n",
       "2    ZVHEZA963      4\n",
       "3    ZSFNU1100      4\n",
       "4    ZRXUB1049      0\n",
       "..         ...    ...\n",
       "422  AGHXWX765      0\n",
       "423  AFEOPC672      3\n",
       "424  AEEEIG737      3\n",
       "425  ADQRPH513      3\n",
       "426  ABNTSS552      4\n",
       "\n",
       "[427 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "filepath = \"data/train.csv\"\n",
    "class_df = pd.read_csv(\n",
    "    filepath, usecols=[1, 2], header=0, names=[\"uid\", \"class\"]\n",
    ")\n",
    "\n",
    "display(class_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>-0.001031</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>-0.113097</td>\n",
       "      <td>-0.284965</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.271864</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680631</td>\n",
       "      <td>-1.153061</td>\n",
       "      <td>0.111816</td>\n",
       "      <td>0.162622</td>\n",
       "      <td>-1.085265</td>\n",
       "      <td>-0.657002</td>\n",
       "      <td>-1.406191</td>\n",
       "      <td>2.240085</td>\n",
       "      <td>0.118616</td>\n",
       "      <td>-0.728013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>-0.045820</td>\n",
       "      <td>-0.216762</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>-0.001331</td>\n",
       "      <td>-0.465898</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.241972</td>\n",
       "      <td>-0.115316</td>\n",
       "      <td>-0.411191</td>\n",
       "      <td>0.431461</td>\n",
       "      <td>0.442649</td>\n",
       "      <td>1.243681</td>\n",
       "      <td>-0.151721</td>\n",
       "      <td>0.458508</td>\n",
       "      <td>1.931918</td>\n",
       "      <td>-0.241081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.083430</td>\n",
       "      <td>-0.292385</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.236576</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659314</td>\n",
       "      <td>-0.792833</td>\n",
       "      <td>-0.471358</td>\n",
       "      <td>0.514799</td>\n",
       "      <td>-0.846220</td>\n",
       "      <td>0.479314</td>\n",
       "      <td>-0.730218</td>\n",
       "      <td>1.352716</td>\n",
       "      <td>0.040223</td>\n",
       "      <td>-0.163302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>-0.109248</td>\n",
       "      <td>-0.183284</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.001447</td>\n",
       "      <td>-0.066267</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047666</td>\n",
       "      <td>-0.201043</td>\n",
       "      <td>-0.565545</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>-0.332314</td>\n",
       "      <td>-0.066972</td>\n",
       "      <td>-1.263785</td>\n",
       "      <td>3.876905</td>\n",
       "      <td>-0.397950</td>\n",
       "      <td>-0.693763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.068301</td>\n",
       "      <td>-0.283487</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>-0.251112</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221178</td>\n",
       "      <td>-0.253239</td>\n",
       "      <td>-0.046740</td>\n",
       "      <td>0.242367</td>\n",
       "      <td>-0.379724</td>\n",
       "      <td>-0.893249</td>\n",
       "      <td>-0.957397</td>\n",
       "      <td>1.118245</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.024197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>-0.093583</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-0.367352</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260746</td>\n",
       "      <td>-0.741712</td>\n",
       "      <td>-0.887129</td>\n",
       "      <td>0.190525</td>\n",
       "      <td>0.216271</td>\n",
       "      <td>0.490549</td>\n",
       "      <td>-1.047399</td>\n",
       "      <td>1.875185</td>\n",
       "      <td>0.345561</td>\n",
       "      <td>-0.874318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.006178</td>\n",
       "      <td>-0.000811</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>-0.108863</td>\n",
       "      <td>-0.302020</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>-0.197981</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457373</td>\n",
       "      <td>-0.782917</td>\n",
       "      <td>-1.072765</td>\n",
       "      <td>1.180279</td>\n",
       "      <td>-0.111142</td>\n",
       "      <td>1.897755</td>\n",
       "      <td>-0.902370</td>\n",
       "      <td>0.552967</td>\n",
       "      <td>-0.314270</td>\n",
       "      <td>-1.198762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>-0.152744</td>\n",
       "      <td>-0.355706</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>-0.320724</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>0.232481</td>\n",
       "      <td>-0.527885</td>\n",
       "      <td>-0.305296</td>\n",
       "      <td>-0.189008</td>\n",
       "      <td>-0.592684</td>\n",
       "      <td>-1.144780</td>\n",
       "      <td>3.459698</td>\n",
       "      <td>-0.199579</td>\n",
       "      <td>-0.999165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>-0.092386</td>\n",
       "      <td>-0.434045</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>-0.228858</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147889</td>\n",
       "      <td>1.168724</td>\n",
       "      <td>-0.486698</td>\n",
       "      <td>1.134707</td>\n",
       "      <td>-0.029372</td>\n",
       "      <td>0.092189</td>\n",
       "      <td>-0.791921</td>\n",
       "      <td>1.786787</td>\n",
       "      <td>2.089036</td>\n",
       "      <td>-0.690614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.047733</td>\n",
       "      <td>-0.071875</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>-0.193959</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064665</td>\n",
       "      <td>-0.444786</td>\n",
       "      <td>-0.879349</td>\n",
       "      <td>1.048909</td>\n",
       "      <td>0.213126</td>\n",
       "      <td>1.170847</td>\n",
       "      <td>-1.172747</td>\n",
       "      <td>1.686595</td>\n",
       "      <td>0.482523</td>\n",
       "      <td>-0.440434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.000462  0.005583 -0.001031  0.002307 -0.113097 -0.284965  0.001069   \n",
       "1    0.000220  0.006780 -0.000547  0.002183 -0.045820 -0.216762  0.000987   \n",
       "2    0.000405  0.007183 -0.000137  0.002612 -0.083430 -0.292385  0.001094   \n",
       "3    0.000388  0.003802  0.002121  0.001513 -0.109248 -0.183284  0.000813   \n",
       "4    0.000425  0.006544  0.001630  0.001549 -0.068301 -0.283487  0.001004   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "422  0.000305  0.003671 -0.004093  0.003010 -0.093583  0.133018  0.000627   \n",
       "423  0.000441  0.006178 -0.000811  0.003572 -0.108863 -0.302020  0.000761   \n",
       "424  0.000464  0.006611  0.000842  0.001412 -0.152744 -0.355706  0.000906   \n",
       "425  0.000233  0.003029  0.001606  0.001224 -0.092386 -0.434045  0.000668   \n",
       "426  0.000233  0.006601 -0.001318  0.001098 -0.047733 -0.071875  0.000654   \n",
       "\n",
       "         7         8         9     ...      1014      1015      1016  \\\n",
       "0   -0.000092 -0.271864  0.000503  ...  0.680631 -1.153061  0.111816   \n",
       "1   -0.001331 -0.465898  0.000515  ... -1.241972 -0.115316 -0.411191   \n",
       "2   -0.000112 -0.236576  0.000466  ...  0.659314 -0.792833 -0.471358   \n",
       "3   -0.001447 -0.066267  0.000654  ... -0.047666 -0.201043 -0.565545   \n",
       "4   -0.001800 -0.251112  0.000428  ... -1.221178 -0.253239 -0.046740   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "422  0.001443 -0.367352  0.000462  ... -0.260746 -0.741712 -0.887129   \n",
       "423  0.001851 -0.197981  0.000310  ...  0.457373 -0.782917 -1.072765   \n",
       "424 -0.001229 -0.320724  0.000493  ...  0.411773  0.232481 -0.527885   \n",
       "425 -0.000410 -0.228858  0.000444  ... -0.147889  1.168724 -0.486698   \n",
       "426  0.000902 -0.193959  0.000319  ... -0.064665 -0.444786 -0.879349   \n",
       "\n",
       "         1017      1018      1019      1020      1021      1022      1023  \n",
       "0    0.162622 -1.085265 -0.657002 -1.406191  2.240085  0.118616 -0.728013  \n",
       "1    0.431461  0.442649  1.243681 -0.151721  0.458508  1.931918 -0.241081  \n",
       "2    0.514799 -0.846220  0.479314 -0.730218  1.352716  0.040223 -0.163302  \n",
       "3    0.999009 -0.332314 -0.066972 -1.263785  3.876905 -0.397950 -0.693763  \n",
       "4    0.242367 -0.379724 -0.893249 -0.957397  1.118245  0.181925 -0.024197  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "422  0.190525  0.216271  0.490549 -1.047399  1.875185  0.345561 -0.874318  \n",
       "423  1.180279 -0.111142  1.897755 -0.902370  0.552967 -0.314270 -1.198762  \n",
       "424 -0.305296 -0.189008 -0.592684 -1.144780  3.459698 -0.199579 -0.999165  \n",
       "425  1.134707 -0.029372  0.092189 -0.791921  1.786787  2.089036 -0.690614  \n",
       "426  1.048909  0.213126  1.170847 -1.172747  1.686595  0.482523 -0.440434  \n",
       "\n",
       "[427 rows x 1024 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load features from individual CSVs into a single dataframe\n",
    "def get_features(uid):\n",
    "    feature_filepath = f\"data/img_details/{uid}.csv\"\n",
    "    feature_df = pd.read_csv(feature_filepath, header=None)\n",
    "    return feature_df.iloc[0].values.tolist()\n",
    "\n",
    "\n",
    "features_df = class_df[[\"uid\"]].apply(\n",
    "    lambda row: get_features(row[0]), axis=1, result_type=\"expand\"\n",
    ")\n",
    "display(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>class</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f1014</th>\n",
       "      <th>f1015</th>\n",
       "      <th>f1016</th>\n",
       "      <th>f1017</th>\n",
       "      <th>f1018</th>\n",
       "      <th>f1019</th>\n",
       "      <th>f1020</th>\n",
       "      <th>f1021</th>\n",
       "      <th>f1022</th>\n",
       "      <th>f1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZYURRE527</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>-0.001031</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>-0.113097</td>\n",
       "      <td>-0.284965</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680631</td>\n",
       "      <td>-1.153061</td>\n",
       "      <td>0.111816</td>\n",
       "      <td>0.162622</td>\n",
       "      <td>-1.085265</td>\n",
       "      <td>-0.657002</td>\n",
       "      <td>-1.406191</td>\n",
       "      <td>2.240085</td>\n",
       "      <td>0.118616</td>\n",
       "      <td>-0.728013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZWNWBP435</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>-0.045820</td>\n",
       "      <td>-0.216762</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>-0.001331</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.241972</td>\n",
       "      <td>-0.115316</td>\n",
       "      <td>-0.411191</td>\n",
       "      <td>0.431461</td>\n",
       "      <td>0.442649</td>\n",
       "      <td>1.243681</td>\n",
       "      <td>-0.151721</td>\n",
       "      <td>0.458508</td>\n",
       "      <td>1.931918</td>\n",
       "      <td>-0.241081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZVHEZA963</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.083430</td>\n",
       "      <td>-0.292385</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659314</td>\n",
       "      <td>-0.792833</td>\n",
       "      <td>-0.471358</td>\n",
       "      <td>0.514799</td>\n",
       "      <td>-0.846220</td>\n",
       "      <td>0.479314</td>\n",
       "      <td>-0.730218</td>\n",
       "      <td>1.352716</td>\n",
       "      <td>0.040223</td>\n",
       "      <td>-0.163302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZSFNU1100</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>-0.109248</td>\n",
       "      <td>-0.183284</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.001447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047666</td>\n",
       "      <td>-0.201043</td>\n",
       "      <td>-0.565545</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>-0.332314</td>\n",
       "      <td>-0.066972</td>\n",
       "      <td>-1.263785</td>\n",
       "      <td>3.876905</td>\n",
       "      <td>-0.397950</td>\n",
       "      <td>-0.693763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZRXUB1049</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.068301</td>\n",
       "      <td>-0.283487</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221178</td>\n",
       "      <td>-0.253239</td>\n",
       "      <td>-0.046740</td>\n",
       "      <td>0.242367</td>\n",
       "      <td>-0.379724</td>\n",
       "      <td>-0.893249</td>\n",
       "      <td>-0.957397</td>\n",
       "      <td>1.118245</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.024197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>AGHXWX765</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>-0.093583</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260746</td>\n",
       "      <td>-0.741712</td>\n",
       "      <td>-0.887129</td>\n",
       "      <td>0.190525</td>\n",
       "      <td>0.216271</td>\n",
       "      <td>0.490549</td>\n",
       "      <td>-1.047399</td>\n",
       "      <td>1.875185</td>\n",
       "      <td>0.345561</td>\n",
       "      <td>-0.874318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>AFEOPC672</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.006178</td>\n",
       "      <td>-0.000811</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>-0.108863</td>\n",
       "      <td>-0.302020</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457373</td>\n",
       "      <td>-0.782917</td>\n",
       "      <td>-1.072765</td>\n",
       "      <td>1.180279</td>\n",
       "      <td>-0.111142</td>\n",
       "      <td>1.897755</td>\n",
       "      <td>-0.902370</td>\n",
       "      <td>0.552967</td>\n",
       "      <td>-0.314270</td>\n",
       "      <td>-1.198762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>AEEEIG737</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>-0.152744</td>\n",
       "      <td>-0.355706</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>0.232481</td>\n",
       "      <td>-0.527885</td>\n",
       "      <td>-0.305296</td>\n",
       "      <td>-0.189008</td>\n",
       "      <td>-0.592684</td>\n",
       "      <td>-1.144780</td>\n",
       "      <td>3.459698</td>\n",
       "      <td>-0.199579</td>\n",
       "      <td>-0.999165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>ADQRPH513</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>-0.092386</td>\n",
       "      <td>-0.434045</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147889</td>\n",
       "      <td>1.168724</td>\n",
       "      <td>-0.486698</td>\n",
       "      <td>1.134707</td>\n",
       "      <td>-0.029372</td>\n",
       "      <td>0.092189</td>\n",
       "      <td>-0.791921</td>\n",
       "      <td>1.786787</td>\n",
       "      <td>2.089036</td>\n",
       "      <td>-0.690614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>ABNTSS552</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.047733</td>\n",
       "      <td>-0.071875</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064665</td>\n",
       "      <td>-0.444786</td>\n",
       "      <td>-0.879349</td>\n",
       "      <td>1.048909</td>\n",
       "      <td>0.213126</td>\n",
       "      <td>1.170847</td>\n",
       "      <td>-1.172747</td>\n",
       "      <td>1.686595</td>\n",
       "      <td>0.482523</td>\n",
       "      <td>-0.440434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid  class        f0        f1        f2        f3        f4  \\\n",
       "0    ZYURRE527      4  0.000462  0.005583 -0.001031  0.002307 -0.113097   \n",
       "1    ZWNWBP435      0  0.000220  0.006780 -0.000547  0.002183 -0.045820   \n",
       "2    ZVHEZA963      4  0.000405  0.007183 -0.000137  0.002612 -0.083430   \n",
       "3    ZSFNU1100      4  0.000388  0.003802  0.002121  0.001513 -0.109248   \n",
       "4    ZRXUB1049      0  0.000425  0.006544  0.001630  0.001549 -0.068301   \n",
       "..         ...    ...       ...       ...       ...       ...       ...   \n",
       "422  AGHXWX765      0  0.000305  0.003671 -0.004093  0.003010 -0.093583   \n",
       "423  AFEOPC672      3  0.000441  0.006178 -0.000811  0.003572 -0.108863   \n",
       "424  AEEEIG737      3  0.000464  0.006611  0.000842  0.001412 -0.152744   \n",
       "425  ADQRPH513      3  0.000233  0.003029  0.001606  0.001224 -0.092386   \n",
       "426  ABNTSS552      4  0.000233  0.006601 -0.001318  0.001098 -0.047733   \n",
       "\n",
       "           f5        f6        f7  ...     f1014     f1015     f1016  \\\n",
       "0   -0.284965  0.001069 -0.000092  ...  0.680631 -1.153061  0.111816   \n",
       "1   -0.216762  0.000987 -0.001331  ... -1.241972 -0.115316 -0.411191   \n",
       "2   -0.292385  0.001094 -0.000112  ...  0.659314 -0.792833 -0.471358   \n",
       "3   -0.183284  0.000813 -0.001447  ... -0.047666 -0.201043 -0.565545   \n",
       "4   -0.283487  0.001004 -0.001800  ... -1.221178 -0.253239 -0.046740   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "422  0.133018  0.000627  0.001443  ... -0.260746 -0.741712 -0.887129   \n",
       "423 -0.302020  0.000761  0.001851  ...  0.457373 -0.782917 -1.072765   \n",
       "424 -0.355706  0.000906 -0.001229  ...  0.411773  0.232481 -0.527885   \n",
       "425 -0.434045  0.000668 -0.000410  ... -0.147889  1.168724 -0.486698   \n",
       "426 -0.071875  0.000654  0.000902  ... -0.064665 -0.444786 -0.879349   \n",
       "\n",
       "        f1017     f1018     f1019     f1020     f1021     f1022     f1023  \n",
       "0    0.162622 -1.085265 -0.657002 -1.406191  2.240085  0.118616 -0.728013  \n",
       "1    0.431461  0.442649  1.243681 -0.151721  0.458508  1.931918 -0.241081  \n",
       "2    0.514799 -0.846220  0.479314 -0.730218  1.352716  0.040223 -0.163302  \n",
       "3    0.999009 -0.332314 -0.066972 -1.263785  3.876905 -0.397950 -0.693763  \n",
       "4    0.242367 -0.379724 -0.893249 -0.957397  1.118245  0.181925 -0.024197  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "422  0.190525  0.216271  0.490549 -1.047399  1.875185  0.345561 -0.874318  \n",
       "423  1.180279 -0.111142  1.897755 -0.902370  0.552967 -0.314270 -1.198762  \n",
       "424 -0.305296 -0.189008 -0.592684 -1.144780  3.459698 -0.199579 -0.999165  \n",
       "425  1.134707 -0.029372  0.092189 -0.791921  1.786787  2.089036 -0.690614  \n",
       "426  1.048909  0.213126  1.170847 -1.172747  1.686595  0.482523 -0.440434  \n",
       "\n",
       "[427 rows x 1026 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge dataframes and fix column names\n",
    "num_features = features_df.shape[1]\n",
    "feature_names = [f\"f{i}\" for i in range(num_features)]\n",
    "\n",
    "features_df.columns = feature_names\n",
    "\n",
    "df = pd.concat([class_df, features_df], axis=1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set by class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    135\n",
       "4     92\n",
       "3     66\n",
       "1     46\n",
       "2     23\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set by class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    24\n",
       "4    17\n",
       "3    12\n",
       "1     8\n",
       "2     4\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all = df[feature_names]\n",
    "y_all = df[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=test_percent, random_state=rand_seed, stratify=y_all\n",
    ")\n",
    "\n",
    "# Reset X_train index\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"Training set by class:\")\n",
    "display(y_train.value_counts())\n",
    "print(\"Test set by class:\")\n",
    "display(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data\n",
    "\n",
    "Use StandardScaler from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that will normalize the X_train data\n",
    "def norm_data(X_train):\n",
    "    # Setup scaler\n",
    "    scaler_std = StandardScaler()\n",
    "    #scaler_abs = MaxAbsScaler()\n",
    "    #scaler_minmax = MinMaxScaler()\n",
    "    \n",
    "    # Apply scaling to training data\n",
    "    X_train = scaler_std.fit_transform(X_train)\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43659260889260926\n",
      "0.20171072818755917\n",
      "0.040687217865955375\n"
     ]
    }
   ],
   "source": [
    "# Test norm_data function\n",
    "X_train = norm_data(X_train)\n",
    "\n",
    "# Check mean and std after normalization\n",
    "norm_mean = np.mean(X_train)\n",
    "norm_std = np.std(X_train)\n",
    "norm_var = np.var(X_train)\n",
    "\n",
    "print(norm_mean)\n",
    "print(norm_std)\n",
    "print(norm_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "\n",
    "Standard deviation approach - For each class, features with values more than +/- 3 standard deviations away from the per class per feature mean will be removed.\n",
    "\n",
    "Since we are in multi-dimensional space, we will use the mean and covariance matrices. This will be computed using Mahalanobis distance which is well-suited for multi-dimensional space: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.mahalanobis.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that computes mean, cov matrix, and inv cov matrix\n",
    "def get_mean_cov(X_train):\n",
    "    # Merge dfs\n",
    "    norm_x_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "    norm_df = pd.concat([y_train, norm_x_df], axis=1)\n",
    "\n",
    "    # Compute mean and cov per class per feature\n",
    "    avg_list = []\n",
    "    cov_list = []\n",
    "    inv_cov_list = []\n",
    "    for i in range(5):\n",
    "        # Compute mean\n",
    "        avg = np.mean(norm_df[norm_df[\"class\"]==i][feature_names], axis=0)\n",
    "        avg_list.append(avg)\n",
    "        # Compute cov matrix\n",
    "        cov = np.cov(norm_df[norm_df[\"class\"]==i][feature_names], rowvar=False)\n",
    "        cov_list.append(cov)\n",
    "        # Compute inverse of cov matrix\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "        inv_cov_list.append(inv_cov)\n",
    "    return norm_df, avg_list, inv_cov_list\n",
    "\n",
    "    #display(avg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test get_mean_cov function\n",
    "norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    362.000000\n",
       "mean      42.746361\n",
       "std       28.969791\n",
       "min        1.314860\n",
       "25%       20.756832\n",
       "50%       35.317028\n",
       "75%       56.876198\n",
       "max      143.976839\n",
       "Name: mahalanobis_dist, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which features should be removed (identify outliers based on Mahalanobis dist)\n",
    "# Create function that computes Mahalanobis distance and adds it to norm_df\n",
    "def get_mahalanobis_dist(label, features):\n",
    "    u = avg_list[label]\n",
    "    v = features\n",
    "    vi = inv_cov_list[label]\n",
    "    delta = u - v\n",
    "    m = np.dot(np.dot(delta, vi), delta)\n",
    "    #dist = distance.mahalanobis(u, features, vi)\n",
    "    return np.sqrt(np.abs(m))\n",
    "\n",
    "# Call function for each feature\n",
    "norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "norm_df[\"mahalanobis_dist\"].describe()\n",
    "\n",
    "# Initialize list for distances\n",
    "# dist = np.zeros(norm_df.shape[0])\n",
    "# for i, row in norm_df.iterrows():\n",
    "#     dist[i] = get_mahalanobis_dist(int(row[\"class\"]), row[feature_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances seem high, but we may have a lot of outliers! Plus there are so many features that the sum becomes large. The distance was calculated for all 362 image feature vectors in the training dataset, so we can move on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers\n",
    "def drop_outliers(norm_df, threshold):\n",
    "    thresh = threshold\n",
    "    norm_df.sort_values(by=\"mahalanobis_dist\", ascending=False, inplace=True)\n",
    "    norm_df.reset_index(inplace=True, drop=True)\n",
    "    norm_df.drop(norm_df.index[:int(norm_df.shape[0]*thresh)], inplace=True)\n",
    "    norm_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    290.000000\n",
       "mean      30.784539\n",
       "std       15.618092\n",
       "min        1.314860\n",
       "25%       18.561889\n",
       "50%       27.950168\n",
       "75%       41.555643\n",
       "max       66.632329\n",
       "Name: mahalanobis_dist, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test drop_outliers function\n",
    "norm_df = drop_outliers(norm_df, 0.2)\n",
    "\n",
    "# Print updated descriptive stats\n",
    "norm_df[\"mahalanobis_dist\"].describe()\n",
    "#print(len(norm_df[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class counts after dropping outliers\n",
    "#print(norm_df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that oversamples or undersamples data\n",
    "def resample(sampler, X_train, y_train, name):\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "    # Observe number of classes after resample\n",
    "    #print(f\"Number of samples per class after {name}:\\n{y_train.value_counts()}\")\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test resample function\n",
    "# Setup ADASYN (oversampling)\n",
    "ada = ADASYN(random_state=rand_seed)\n",
    "# Setup random undersampling\n",
    "undersample = RandomUnderSampler()\n",
    "\n",
    "# Call resample function\n",
    "#X_train, y_train = resample(undersample, X_train, y_train, \"Random Undersampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality - Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis to reduce dimensionality of problem\n",
    "def use_lda(X_train, y_train):\n",
    "    lda_clf = LinearDiscriminantAnalysis()\n",
    "    lda_model = lda_clf.fit(X_train, y_train)\n",
    "    X_lda = lda_model.transform(X_train)\n",
    "    return X_lda, lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test use_lda function\n",
    "#X_lda, lda_model = use_lda(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LDA results\n",
    "def plot_lda(X_lda, y_train):\n",
    "    # Visualize LDA results\n",
    "    target_class = np.unique(y_train)\n",
    "    colors = [\"red\", \"green\", \"blue\", \"black\", \"brown\"]\n",
    "    # Get number of components\n",
    "    n_components = X_lda.shape[1]\n",
    "    pairs = []\n",
    "    for xi in range(n_components):\n",
    "        for yi in range(n_components):\n",
    "            if yi > xi:\n",
    "                pairs.append((xi,yi))\n",
    "\n",
    "    # Setup subplots\n",
    "    fig, ax = plt.subplots(len(pairs), 1, figsize=(10,30))\n",
    "\n",
    "    # Get every combination of LDA components\n",
    "    # Setting up all values to plot\n",
    "    for ax_i,(xi,yi) in enumerate(pairs):\n",
    "        # Plot data\n",
    "        for color, i, c in zip(colors, [0,1,2,3,4], target_class):\n",
    "            ax[ax_i].scatter(X_lda[y_train == i, xi],\n",
    "                X_lda[y_train == i, yi],\n",
    "                alpha=.8, color=color, label=c)\n",
    "        # Add legend\n",
    "        ax[ax_i].set_xlabel(f\"Component {xi}\")\n",
    "        ax[ax_i].set_ylabel(f\"Component {yi}\")\n",
    "        ax[ax_i].legend(loc=\"best\")\n",
    "\n",
    "    # Set title on the plot\n",
    "    #plt.title(\"Linear Discriminant Analysis\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plot_lda function\n",
    "#plot_lda(X_lda, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis to reduce dimensionality of problem\n",
    "def use_pca(X_train, y_train):\n",
    "    pca_clf = PCA()\n",
    "    pca_model = pca_clf.fit(X_train, y_train)\n",
    "    X_pca = pca_model.transform(X_train)\n",
    "    return X_pca, pca_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVM Classifier with Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "# PIPELINE 1: NORMALIZE, DROP OUTLIERS, UNDERSAMPLE, LDA\n",
    "# Normalize\n",
    "X_train = norm_data(X_train)\n",
    "normalize_bool = 1\n",
    "\n",
    "# Drop outliers\n",
    "norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)\n",
    "norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "outlier_thresh = 0.2\n",
    "norm_df = drop_outliers(norm_df, outlier_thresh)\n",
    "outlier_bool = 1\n",
    "\n",
    "# Resample\n",
    "undersample = RandomUnderSampler()\n",
    "X_train = norm_df[feature_names]\n",
    "y_train = norm_df[\"class\"]\n",
    "X_train, y_train = resample(undersample, X_train, y_train, \"Random Undersampling\")\n",
    "resample_bool = 1\n",
    "\n",
    "# LDA\n",
    "X_lda, lda_model = use_lda(X_train, y_train)\n",
    "dim_reduction_bool = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 2: NORMALIZE, DROP OUTLIERS, LDA\n",
    "# Normalize\n",
    "X_train = norm_data(X_train)\n",
    "normalize_bool = 1\n",
    "\n",
    "# Drop outliers\n",
    "norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)\n",
    "norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "outlier_thresh = 0.2\n",
    "norm_df = drop_outliers(norm_df, outlier_thresh)\n",
    "outlier_bool = 1\n",
    "\n",
    "# LDA\n",
    "X_lda, lda_model = use_lda(X_train, y_train)\n",
    "dim_reduction_bool = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 3: NORMALIZE, DROP OUTLIERS\n",
    "# Normalize\n",
    "X_train = norm_data(X_train)\n",
    "normalize_bool = 1\n",
    "\n",
    "# Drop outliers\n",
    "norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)\n",
    "norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "outlier_thresh = 0.2\n",
    "norm_df = drop_outliers(norm_df, outlier_thresh)\n",
    "outlier_bool = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 4: NORMALIZE, DROP OUTLIERS, PCA\n",
    "# # Normalize\n",
    "# X_train = norm_data(X_train)\n",
    "# normalize_bool = 1\n",
    "\n",
    "# # Drop outliers\n",
    "# norm_df, avg_list, inv_cov_list = get_mean_cov(X_train)\n",
    "# norm_df[\"mahalanobis_dist\"] = norm_df.apply(lambda row: get_mahalanobis_dist(int(row[\"class\"]), row[feature_names]), axis=1)\n",
    "# outlier_thresh = 0.2\n",
    "# norm_df = drop_outliers(norm_df, outlier_thresh)\n",
    "# outlier_bool = 1\n",
    "\n",
    "# PCA\n",
    "X_pca, pca_model = use_pca(X_train, y_train)\n",
    "dim_reduction_bool=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heath\\anaconda3\\envs\\pattern-classification\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but LinearDiscriminantAnalysis was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run LDA on X test data to match dimensionality of train data\n",
    "#X_test_lda = lda_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on X test data to match dimensionality of train data\n",
    "X_test_pca = pca_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.369\n",
      "F1 score: 0.199\n",
      "Precision: 0.136\n",
      "Recall: 0.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heath\\anaconda3\\envs\\pattern-classification\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\heath\\anaconda3\\envs\\pattern-classification\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Use components to train SVM classifier\n",
    "svm_clf = SVC(kernel=\"rbf\", random_state=10)\n",
    "svm_model = svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Use cross_validate from sklearn to obtain accuracies for 5-fold cross validation\n",
    "#cv_results = cross_validate(svm_clf, X_train, y_train, cv=5,\n",
    "#    scoring='accuracy')\n",
    "\n",
    "# Print accuracy for each fold\n",
    "#for i in range(0,5):\n",
    "#    print(f\"The accuracy for fold {i} is: {cv_results['test_score'][i]:.3f}\")\n",
    "    \n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"F1 score: {f1:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1c064f0a0e0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    F1Score  Accuracy  Precision    Recall  TestSplit  KFold  Normalized  \\\n",
      "0  0.341409  0.384615   0.402385  0.384615       0.15    0.0         0.0   \n",
      "\n",
      "   OutlierRemoval  OutlierThreshold  Resample  ReduceDimension  \n",
      "0             0.0               0.2       0.0              1.0  \n"
     ]
    }
   ],
   "source": [
    "# Save all results to a DataFrame\n",
    "# First, initialize dataframe\n",
    "pipeline_results_df = pd.DataFrame(columns=[\"F1Score\", \"Accuracy\", \"Precision\", \"Recall\",\n",
    "    \"TestSplit\", \"KFold\", \"Normalized\", \"OutlierRemoval\", \"OutlierThreshold\",\n",
    "    \"Resample\", \"ReduceDimension\"])\n",
    "\n",
    "pipeline_results_df.loc[0] = [f1, acc, precision, recall, test_percent, 0, normalize_bool,\n",
    "    outlier_bool, outlier_thresh, resample_bool, dim_reduction_bool]  \n",
    "\n",
    "print(pipeline_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to csv\n",
    "# Check if csv exists\n",
    "file_path = \"outputs/pipeline_results.csv\"\n",
    "if os.path.isfile(file_path):\n",
    "    # Append new results\n",
    "    pipeline_results_df.to_csv(file_path, mode=\"a\", header=False)\n",
    "else:\n",
    "    # Create new csv and add results\n",
    "    pipeline_results_df.to_csv(file_path, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pattern-classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1895724e0ba1d87308f72752509c0d197b6cd14cd38e9b4860259e222d188bca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
